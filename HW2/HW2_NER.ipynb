{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Варианты того, как можно решить эту задачу:\n",
    "\n",
    "- Написать правила с помощью spacy matcher (не очень мне нравится, но работает для английского), используя синтаксические шаблоны (я заметила, что чаще всего в текстах продукты упоминаются в предложениях типа \"I used PRODUCT for...\" или \"This PRODUCT is very...\"). Плюсы: достаточно легко и быстро, найдутся как упоминания названий товаров (I used Dreamweaver for...) так и их дескрипторы (This visual studio is really great...), + из некоторых таких паттернов можно сразу извлечь и нужную информацию об оценке продукта. Минусы: все паттерны мы не охватим все равно, скорее всего выделим что-то лишнее\n",
    "\n",
    "\n",
    "- Использовать какую-нибудь модель для извлечения ключевых слов (например, tf-idf), полагаясь на то, что категории товаров разнородны, а корпус большой, и для каждого текста отзыва название товара/его категория будут ключевыми. Плюсы: опять же выделятся и сами названия продуктов, и их более общие названия (в теории). Минусы: лучше сработает для категории товаров с более разнообразными товарами (для Software посложнее, там синонимов немного). Скорее всего выделится что-то лишнее (частотные слова - не названия продукта, но связанные с ним названия его фичей, например, хотя это тоже может быть полезно, если задача, например, собрать общие оценки пользователей не только по продукту, но и по его отдельным аспектам), нужна дополнительная фильтрация\n",
    "\n",
    "\n",
    "- Собрать список дескрипторов: руками + воспользоваться каким-нибудь тезаурусом типа Ворднет и собрать все гипонимы данной категории (хорошо подходит для конкретных категорий с разными видами продуктов: например, для косметики) и извлекать их из текста. Плюсы: скорее всего, охватим почти все возможные дексрипторы. Минусы: не охватим сами названия брендов (только если они будут в тексте идти вместе - Chanel Perfume)\n",
    "\n",
    "\n",
    "- Использовать готовую NER модель. Плюсы: ничего делать не нужно, инструмент готов. Найдем все упоминания названий товаров. Минусы: модель может совершать ошибки, когда названия сущностей омонимичны сущностям другого рода. Не найдем случаи, когда товар назван просто \"телефон\" и тд.\n",
    "\n",
    "\n",
    "- Комбинация из двух последних способов - объединить предсказания этих двух подходов. Плюсы: соберем и все дескриптивные упоминания товаров, и все их названия. Минусы: не для всех типов товаров подходит подход.\n",
    "\n",
    "Я выбрала первы подход из-за специфики выбранных данных - у товаров этой категории не так много дескрипторов и в целом они не очень разнородны, беглый взгляд на данные как будто подсказывает, что нужно скорее обратить внимание на синтаксис, чем на лексику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, MWETokenizer\n",
    "from nltk.collocations import BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Software_5.json') as file:\n",
    "    raw_data = file.read()\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765e4c890c754323b2c3ea5b06578904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12805.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = raw_data.split('\\n')[:-1]\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for review in tqdm(reviews):\n",
    "    data = data.append(eval(review), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12800</th>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>07 16, 2016</td>\n",
       "      <td>A1E50L7PCVXLN4</td>\n",
       "      <td>B01FFVDY9M</td>\n",
       "      <td>{'Platform:': ' Key Card'}</td>\n",
       "      <td>Colinda</td>\n",
       "      <td>When I ordered this it was listed as Photo Edi...</td>\n",
       "      <td>File Management Software with Basic Editing Ca...</td>\n",
       "      <td>1.468627e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12801</th>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>06 17, 2017</td>\n",
       "      <td>AVU1ILDDYW301</td>\n",
       "      <td>B01HAP3NUG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G. Hearn</td>\n",
       "      <td>This software has SO much going on.  Theres a ...</td>\n",
       "      <td>Might not be for the \"novice\"</td>\n",
       "      <td>1.497658e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12802</th>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>01 24, 2017</td>\n",
       "      <td>A2LW5AL0KQ9P1M</td>\n",
       "      <td>B01HAP3NUG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dr. E</td>\n",
       "      <td>I have used both more complex and less complex...</td>\n",
       "      <td>Great, Inexpensive Software for Those Who Have...</td>\n",
       "      <td>1.485216e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12803</th>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>06 14, 2018</td>\n",
       "      <td>AZ515FFZ7I2P7</td>\n",
       "      <td>B01HAP47PQ</td>\n",
       "      <td>{'Platform:': ' PC Disc'}</td>\n",
       "      <td>Jerry Jackson Jr.</td>\n",
       "      <td>Pinnacle Studio 20 Ultimate is a perfectly ser...</td>\n",
       "      <td>Gets the job done ... but not as easy as it sh...</td>\n",
       "      <td>1.528934e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12804</th>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>04 16, 2018</td>\n",
       "      <td>A2WPL6Y08K6ZQH</td>\n",
       "      <td>B01HAP47PQ</td>\n",
       "      <td>{'Platform:': ' PC Disc'}</td>\n",
       "      <td>Narut Ujnat</td>\n",
       "      <td>A program that is fairly easy to use and provi...</td>\n",
       "      <td>Good overall program.</td>\n",
       "      <td>1.523837e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall verified   reviewTime      reviewerID        asin  \\\n",
       "12800      4.0    False  07 16, 2016  A1E50L7PCVXLN4  B01FFVDY9M   \n",
       "12801      3.0    False  06 17, 2017   AVU1ILDDYW301  B01HAP3NUG   \n",
       "12802      4.0    False  01 24, 2017  A2LW5AL0KQ9P1M  B01HAP3NUG   \n",
       "12803      3.0    False  06 14, 2018   AZ515FFZ7I2P7  B01HAP47PQ   \n",
       "12804      4.0    False  04 16, 2018  A2WPL6Y08K6ZQH  B01HAP47PQ   \n",
       "\n",
       "                            style       reviewerName  \\\n",
       "12800  {'Platform:': ' Key Card'}            Colinda   \n",
       "12801                         NaN           G. Hearn   \n",
       "12802                         NaN              Dr. E   \n",
       "12803   {'Platform:': ' PC Disc'}  Jerry Jackson Jr.   \n",
       "12804   {'Platform:': ' PC Disc'}        Narut Ujnat   \n",
       "\n",
       "                                              reviewText  \\\n",
       "12800  When I ordered this it was listed as Photo Edi...   \n",
       "12801  This software has SO much going on.  Theres a ...   \n",
       "12802  I have used both more complex and less complex...   \n",
       "12803  Pinnacle Studio 20 Ultimate is a perfectly ser...   \n",
       "12804  A program that is fairly easy to use and provi...   \n",
       "\n",
       "                                                 summary  unixReviewTime vote  \\\n",
       "12800  File Management Software with Basic Editing Ca...    1.468627e+09  NaN   \n",
       "12801                      Might not be for the \"novice\"    1.497658e+09  NaN   \n",
       "12802  Great, Inexpensive Software for Those Who Have...    1.485216e+09  NaN   \n",
       "12803  Gets the job done ... but not as easy as it sh...    1.528934e+09  NaN   \n",
       "12804                              Good overall program.    1.523837e+09  NaN   \n",
       "\n",
       "      image  \n",
       "12800   NaN  \n",
       "12801   NaN  \n",
       "12802   NaN  \n",
       "12803   NaN  \n",
       "12804   NaN  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(subset=['reviewText'], inplace=True)\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_texts = data.reviewText.values\n",
    "summ_texts = data.summary.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы лучше понять, какие правила писать, посчитаем самые частотные глаголы в текстах ревью и самые частотные существительные в текстах саммари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2291fbb45724462494ba622d35ba8738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12804.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "verbs_freq = Counter()\n",
    "\n",
    "lemm_reviews_texts = []\n",
    "for review in tqdm(reviews_texts):\n",
    "    doc = nlp(review)\n",
    "    \n",
    "    lemm_review = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        lemm_review.append(lemma) \n",
    "        if token.pos_ == 'VERB':\n",
    "            verbs_freq[lemma] += 1\n",
    "            \n",
    "    lemm_reviews_texts.append(' '.join(lemm_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95abd56274654e79837c857460da5b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12804.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nouns_freq = Counter()\n",
    "\n",
    "lemm_summ_texts = []\n",
    "for summary in tqdm(summ_texts):\n",
    "    if not pd.isna(summary):\n",
    "        doc = nlp(summary)\n",
    "\n",
    "        lemm_summary = []\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            lemm_summary.append(lemma) \n",
    "            if token.pos_ == 'NOUN':\n",
    "                nouns_freq[lemma] += 1\n",
    "\n",
    "        lemm_summ_texts.append(' '.join(lemm_summary))\n",
    "    else:\n",
    "        lemm_summ_texts.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('star', 1336),\n",
       " ('software', 625),\n",
       " ('product', 609),\n",
       " ('program', 315),\n",
       " ('version', 298),\n",
       " ('year', 244),\n",
       " ('price', 220),\n",
       " ('feature', 207),\n",
       " ('time', 190),\n",
       " ('work', 186),\n",
       " ('computer', 172),\n",
       " ('user', 153),\n",
       " ('tax', 149),\n",
       " ('problem', 143),\n",
       " ('upgrade', 134),\n",
       " ('security', 134),\n",
       " ('video', 130),\n",
       " ('pc', 129),\n",
       " ('protection', 126),\n",
       " ('money', 118),\n",
       " ('way', 113),\n",
       " ('lot', 108),\n",
       " ('editing', 102),\n",
       " ('tool', 99),\n",
       " ('office', 94),\n",
       " ('update', 91),\n",
       " ('support', 90),\n",
       " ('business', 86),\n",
       " ('value', 85),\n",
       " ('alternative', 81),\n",
       " ('device', 79),\n",
       " ('home', 78),\n",
       " ('job', 77),\n",
       " ('improvement', 75),\n",
       " ('issue', 73),\n",
       " ('thing', 70),\n",
       " ('fun', 68),\n",
       " ('review', 68),\n",
       " ('taxis', 68),\n",
       " ('internet', 67),\n",
       " ('file', 64),\n",
       " ('photo', 60),\n",
       " ('package', 59),\n",
       " ('bit', 57),\n",
       " ('need', 55),\n",
       " ('system', 55),\n",
       " ('install', 55),\n",
       " ('interface', 54),\n",
       " ('suite', 53),\n",
       " ('switch', 51)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_freq.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('use', 16696),\n",
       " ('can', 11897),\n",
       " ('will', 9406),\n",
       " ('work', 6888),\n",
       " ('make', 5870),\n",
       " ('would', 5325),\n",
       " ('go', 5169),\n",
       " ('find', 5069),\n",
       " ('need', 5030),\n",
       " ('get', 4512),\n",
       " ('want', 4468),\n",
       " ('run', 4321),\n",
       " ('take', 4132),\n",
       " ('try', 3690),\n",
       " ('instal', 3000),\n",
       " ('give', 2967),\n",
       " ('say', 2918),\n",
       " ('come', 2907),\n",
       " ('buy', 2906),\n",
       " ('may', 2895),\n",
       " ('think', 2860),\n",
       " ('have', 2799),\n",
       " ('could', 2770),\n",
       " ('see', 2749),\n",
       " ('like', 2712),\n",
       " ('include', 2710),\n",
       " ('look', 2705),\n",
       " ('know', 2576),\n",
       " ('seem', 2522),\n",
       " ('do', 2415),\n",
       " ('learn', 2360),\n",
       " ('install', 2283),\n",
       " ('start', 2153),\n",
       " ('recommend', 2026),\n",
       " ('create', 2020),\n",
       " ('should', 1900),\n",
       " ('keep', 1888),\n",
       " ('add', 1877),\n",
       " ('upgrade', 1801),\n",
       " ('allow', 1656),\n",
       " ('download', 1578),\n",
       " ('offer', 1442),\n",
       " ('pay', 1374),\n",
       " ('save', 1355),\n",
       " ('set', 1319),\n",
       " ('link', 1297),\n",
       " ('provide', 1285),\n",
       " ('update', 1283),\n",
       " ('let', 1265),\n",
       " ('help', 1248)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_freq.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем некоторое кооличество правил с помощью spacy matcher, которые помогут нам достать как общие названия типов продуктов, так и их бренды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#  I used Microsoft Office, I liked this product\n",
    "pattern1 = [{\"LEMMA\": {\"IN\": [\"use\", \"like\", \"instal\"]}}, \n",
    "            {\"lower\": \"this\", \"OP\": \"*\"}, \n",
    "            {\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "\n",
    "#  This player is amazing, This swith has useful features...\n",
    "pattern2 = [{\"lower\": \"this\"}, \n",
    "            {\"POS\": {\"IN\": [\"PROPN\", \"NOUN\"]}, \"OP\": \"+\"}, \n",
    "            {\"LEMMA\": {\"IN\": [\"be\", \"have\"]}}, \n",
    "            {\"POS\": \"ADJ\", \"OP\": \"*\"}]\n",
    "\n",
    "\n",
    "#  Nancy Drew game,  Verbarrator software etc.\n",
    "pattern3 = [{\"POS\": \"PROPN\"}, \n",
    "            {\"POS\": \"PROPN\", \"OP\": \"*\"}, \n",
    "            {\"lower\": {\"IN\":[\"program\", \"software\", \"player\", \"package\", \"tool\", \"game\"]}}]\n",
    "\n",
    "matcher.add(\"verb_pattern\", [pattern1])\n",
    "matcher.add(\"this_pattern\", [pattern2])\n",
    "matcher.add(\"descriptor_pattern\", [pattern3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем парсинг и выделяем сами строки с упоминаниями товаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(text):\n",
    "    spans = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        \n",
    "        spans.append(doc[start:end])\n",
    "          \n",
    "    filt_spans = filter_spans(spans)\n",
    "    \n",
    "    return filt_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_products(match):\n",
    "    tokens = [token.text.lower() for token in match]\n",
    "    \n",
    "    if tokens[0] in [\"use\", \"like\", \"instal\"]:\n",
    "        product = ' '.join(tokens[1:])\n",
    "        \n",
    "    elif 'this' in tokens:\n",
    "        this_ind = tokens.index('this')\n",
    "        \n",
    "        if 'be' in tokens:\n",
    "            verb_ind = tokens.index('be')\n",
    "        elif 'have' in tokens:\n",
    "            verb_ind = tokens.index('have')\n",
    "        else:\n",
    "            print(tokens)\n",
    "            return None\n",
    "            \n",
    "        product = ' '.join(tokens[this_ind+1 : verb_ind])\n",
    "        \n",
    "    elif tokens[-1] in [\"program\", \"software\", \"player\", \"package\", \"tool\", \"game\"]:\n",
    "        product = ' '.join(tokens)\n",
    "        \n",
    "    else:\n",
    "        print(tokens)\n",
    "        return None\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products_mentions(text):\n",
    "    spans = get_spans(text)\n",
    "    \n",
    "    all_prodnames = []\n",
    "    \n",
    "    for ind, span in enumerate(spans):\n",
    "        mention = extract_products(span)\n",
    "        if mention:\n",
    "            all_prodnames.append(mention)\n",
    "    return all_prodnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481f2b467c9e4ac7b0ca0616ede96de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12804.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['installed', 'microsoft', 'visual', 'studio']\n",
      "['used', 'snoop']\n",
      "['this', 'software.i', 'am']\n",
      "['this', 'program']\n",
      "['used', 'os']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_all = []\n",
    "\n",
    "for text in tqdm(lemm_reviews_texts):\n",
    "    extracted = get_products_mentions(text)\n",
    "    extracted_all.append(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В распечатанных примерах что-то пошло не так из-за ошибок лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_all = sum(extracted_all, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dreamweaver',\n",
       " 'course',\n",
       " 'courseware',\n",
       " 'course',\n",
       " 'flash files',\n",
       " 'flash video',\n",
       " 'ap',\n",
       " 'spry',\n",
       " 'dw',\n",
       " 'dreamweaver',\n",
       " 'tutorial',\n",
       " 'dreamweaver',\n",
       " 'spry',\n",
       " 'dreamweaver',\n",
       " 'front page',\n",
       " 'adobe lightroom',\n",
       " 'adobe bridge',\n",
       " 'lightroom',\n",
       " 'video series',\n",
       " 'photoshop',\n",
       " 'flash cs5',\n",
       " 'ms windows',\n",
       " 'flash cs5',\n",
       " 'flash tool',\n",
       " 'software',\n",
       " 'office',\n",
       " 'year',\n",
       " 'office products',\n",
       " 'ms office',\n",
       " 'ms office software',\n",
       " 'package',\n",
       " 'office',\n",
       " 'iti',\n",
       " 'microsoft package',\n",
       " 'office program',\n",
       " 'office version',\n",
       " 'outlook',\n",
       " 'office software',\n",
       " 'review',\n",
       " 'version',\n",
       " 'outlook',\n",
       " 'google drive',\n",
       " 'microsoft office',\n",
       " 'thing',\n",
       " 'gmail',\n",
       " 'outlook',\n",
       " 'office',\n",
       " 'office',\n",
       " 'office',\n",
       " 'office']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_all[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделенные сущности выглядят неплохо. Токенизируем тексты по-новому (с учетом выделенных биграмм), выделяем все биграммы из тексты и оставляем только те, в которых есть наши сущности + не-пунктуация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  токенизатор, который не будет разделять сущности из двух слов (при подсчете биграм будем считать их за один токен)\n",
    "tokenizer_mwe = MWETokenizer(separator=\" \")\n",
    "\n",
    "for entity in extracted_all:\n",
    "    tokenizer_mwe.add_mwe(tuple(entity.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c462f5809244eae8889340a2b1186cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12804.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bigrams_counter = Counter()\n",
    "tokenized_texts = []\n",
    "\n",
    "for text in tqdm(reviews_texts):\n",
    "    tokens = tokenizer_mwe.tokenize(word_tokenize(text.lower()))\n",
    "    tokenized_texts.append(tokens)\n",
    "    \n",
    "    bigrams_text = list(nltk.bigrams(tokens))\n",
    "    filtered_bigrams = []\n",
    "    \n",
    "    for bigram in bigrams_text:\n",
    "        if bigram[0] not in string.punctuation and bigram[1] not in string.punctuation:\n",
    "            \n",
    "            if bigram[0] in extracted_all:\n",
    "                filtered_bigrams.append(bigram)\n",
    "                \n",
    "            elif bigram[1] in extracted_all:\n",
    "                filtered_bigrams.append(bigram)\n",
    "   \n",
    "    bigrams_counter.update(filtered_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92505"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bigrams = bigrams_counter.keys()\n",
    "len(all_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем PMI для всех биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_documents(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931cfc46f32540c29e182694664baa91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=92505.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pmi_scores = []\n",
    "loglike_scores = []\n",
    "t_scores = []\n",
    "\n",
    "for bigram in tqdm(all_bigrams):\n",
    "    pmi = finder.score_ngram(bigram_measures.pmi, bigram[0], bigram[1])\n",
    "    pmi_scores.append((bigram, pmi))\n",
    "    \n",
    "    loglike = finder.score_ngram(bigram_measures.likelihood_ratio, bigram[0], bigram[1])\n",
    "    loglike_scores.append((bigram, loglike))\n",
    "    \n",
    "    tscore = finder.score_ngram(bigram_measures.student_t, bigram[0], bigram[1])\n",
    "    t_scores.append((bigram, tscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем все в отдельный датафрейм, группируем по сущности и оставляем в каждой группе только 5 первых результатов. Сущностей выделено много, поэтому сохраняем данные в отдельный csv-файл (лежит в репозитории), чтобы анализировать результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_group(bigram):\n",
    "    if bigram[0] in extracted_all:\n",
    "        return bigram[0]\n",
    "    elif bigram[1] in extracted_all:\n",
    "        return bigram[1]\n",
    "\n",
    "res_df = pd.DataFrame()\n",
    "res_df['bigram'] = [b[0] for b in pmi_scores]\n",
    "res_df['pmi'] = [b[1] for b in pmi_scores]\n",
    "res_df['loglike'] = [b[1] for b in loglike_scores]\n",
    "res_df['tscore'] = [b[1] for b in t_scores]\n",
    "res_df['item_group'] = res_df['bigram'].apply(get_item_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = res_df[['item_group', 'bigram', 'pmi']].groupby('item_group').apply(lambda x: x.sort_values('pmi', ascending = False)).reset_index(drop=True)\n",
    "res_pmi = new.groupby('item_group').head(5)\n",
    "\n",
    "new = res_df[['item_group', 'bigram', 'loglike']].groupby('item_group').apply(lambda x: x.sort_values('loglike', ascending = False)).reset_index(drop=True)\n",
    "res_loglike = new.groupby('item_group').head(5)\n",
    "\n",
    "new = res_df[['item_group', 'bigram', 'tscore']].groupby('item_group').apply(lambda x: x.sort_values('tscore', ascending = False)).reset_index(drop=True)\n",
    "res_tscore = new.groupby('item_group').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pmi.to_csv('pmi_result.csv')\n",
    "res_loglike.to_csv('loglike_result.csv')\n",
    "res_tscore.to_csv('tscore_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-score плохо справляется с задачей и больше всего выделяет сочетание сущностей с частотными стоп-словами, чуть лучше, но все еще не очень хорошие результаты у loglikelihood, PMI справляется лучше остальных и для более частотных сущностей иногда выделяется что-то полезное (('functional', 'accounting software'), ('wonderful', 'adobe photoelements program'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи разрешения синонимии типа \"watch\", \"smartwatch\" я бы считала jaccard_similarity_score или что-то похоже дляя выделенных сущностей в рамках одного текста, и если они пересекают определенный порог этой близости, считала бы их относящимися к одной сущности (но реализовать это не успела) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
