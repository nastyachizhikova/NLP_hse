{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание по Автобрее №2\n",
    "## Настя Чижикова, БКЛ181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Русский текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве русского текста я взяла какой-то бугурт-тред из ВКонтакте (см. файл 'russian_text.txt'): в нем много сленговых слов, но также он частеречно разнообразен. Вручную расставила части речи: для каждого слова через нижнее подчеркивание его часть речи (дом_NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('russian_text.txt', encoding='utf-8') as f:\n",
    "    rus_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним в словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {}\n",
    "\n",
    "for w in nltk.word_tokenize(rus_text):\n",
    "    if w[0].isalpha():\n",
    "        word, pos = w.split('_')\n",
    "        pos_dict[word] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Парсинг PyMorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "pym_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='как', tag=OpencorporaTag('CONJ'), normal_form='как', score=0.875, methods_stack=((<DictionaryAnalyzer>, 'как', 1673, 0),)),\n",
       " Parse(word='как', tag=OpencorporaTag('ADVB,Ques'), normal_form='как', score=0.09375, methods_stack=((<DictionaryAnalyzer>, 'как', 1674, 0),)),\n",
       " Parse(word='как', tag=OpencorporaTag('PRCL'), normal_form='как', score=0.03125, methods_stack=((<DictionaryAnalyzer>, 'как', 22, 0),))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.parse('как')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in nltk.word_tokenize(rus_text):\n",
    "    word = w.split('_')[0]\n",
    "    if word.isalpha():\n",
    "        pos = m.parse(word)[0].tag.POS\n",
    "        pym_dict[word] = pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MyStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "mys_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 272/272 [05:44<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "for w in tqdm(nltk.word_tokenize(rus_text)):  #  токенизируем везде одинаково, чтобы легче сравнить части речи\n",
    "    word = w.split('_')[0]\n",
    "    if word.isalpha():\n",
    "        an = m.analyze(word)[0]['analysis']\n",
    "        if an == []:  #  в случае, если MyStem не может определить часть речи\n",
    "            pos = 'None'\n",
    "        else:\n",
    "            pos = an[0]['gr'].split(',')[0]\n",
    "        \n",
    "        mys_dict[word] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ПОПАДАЕШЬ': 'V',\n",
       " 'В': 'S',\n",
       " 'ЛОББИ': 'S',\n",
       " 'РЕГИСТРИРУЕШЬСЯ': 'V',\n",
       " 'И': 'PART=',\n",
       " 'РАДОСТНО': 'ADV=',\n",
       " 'СКАЧЕШЬ': 'V',\n",
       " 'К': 'S',\n",
       " 'ПОРТАЛУ': 'S',\n",
       " 'С': 'S',\n",
       " 'НАДПИСЬЮ': 'S',\n",
       " 'ВЫЖИВАНИЕ': 'S',\n",
       " 'ПРЕДВКУШАЯ': 'V=непрош',\n",
       " 'КАК': 'ADVPRO=',\n",
       " 'ВЫКОПАЕШЬ': 'V',\n",
       " 'ВСЕ': 'SPRO',\n",
       " 'АЛМАЗЫ': 'S',\n",
       " 'МИРА': 'S',\n",
       " 'УШАТАЕШЬ': 'V',\n",
       " 'ВСЕХ': 'SPRO',\n",
       " 'НА': 'PART=',\n",
       " 'ПВП': 'None',\n",
       " 'ТЕБЯ': 'SPRO',\n",
       " 'ВСТРЕЧАЕТ': 'V',\n",
       " 'НЕБОЛЬШОЙ': 'A=(вин',\n",
       " 'СПАУН': 'S',\n",
       " 'ПРЕДСТАВЛЯЮЩИЙ': 'V',\n",
       " 'ИЗ': 'PR=',\n",
       " 'СЕБЯ': 'SPRO=(вин|род)',\n",
       " 'ПРЯМОУГОЛЬНЫЙ': 'A=(вин',\n",
       " 'КАМЕННЫЙ': 'A=(вин',\n",
       " 'ЗАБОР': 'S',\n",
       " 'ВНУТРИ': 'ADV=',\n",
       " 'КОТОРОГО': 'APRO=(вин',\n",
       " 'НАТЫКАНЫ': 'V',\n",
       " 'ДОРОГИ': 'S',\n",
       " 'ПРИМИТИВНЫЕ': 'A=(вин',\n",
       " 'ПОСТРОЙКИ': 'S',\n",
       " 'ПОД': 'S',\n",
       " 'ВАРПЫ': 'S',\n",
       " 'НЕМНОГО': 'ADV=',\n",
       " 'РАЗОЧАРОВАВШИСЬ': 'V',\n",
       " 'КРЕАТИВНОСТИ': 'S',\n",
       " 'МЕСТНОГО': 'A=(вин',\n",
       " 'АДМИНА': 'S',\n",
       " 'НАЖИМАЕШЬ': 'V',\n",
       " 'КНОПКУ': 'S',\n",
       " 'ОХЕРЕВАЕШЬ': 'V=непрош',\n",
       " 'ОТ': 'PR=',\n",
       " 'ФАКТА': 'S',\n",
       " 'ЧТО': 'SPRO',\n",
       " 'ТВОЙ': 'APRO=(вин',\n",
       " 'ИНВЕНТАРЬ': 'S',\n",
       " 'НУЛЕВОЙ': 'A=(вин',\n",
       " 'СЕКУНДЫ': 'S',\n",
       " 'БЫЛ': 'V',\n",
       " 'ЗАБИТ': 'V',\n",
       " 'СТРОИТЕЛЬНЫМИ': 'A=твор',\n",
       " 'РЕСУРСАМИ': 'S',\n",
       " 'ЕДОЙ': 'S',\n",
       " 'ИНСТРУМЕНТАМИ': 'S',\n",
       " 'ОРУЖИЕМ': 'S',\n",
       " 'БРОНЁЙ': 'S',\n",
       " 'ОСМОТРЕВШИСЬ': 'V',\n",
       " 'ПОНИМАЕШЬ': 'V=непрош',\n",
       " 'ПОВСЮДУ': 'ADV=',\n",
       " 'ЛЕТАЮТ': 'V',\n",
       " 'ДОДИКИ': 'S',\n",
       " 'КРЕАТИВЕ': 'S',\n",
       " 'РАЗБРАСЫВАЮТ': 'V=непрош',\n",
       " 'ШАЛКЕРЫ': 'S',\n",
       " 'НАБИТЫЕ': 'V',\n",
       " 'ТОПОВЫМИ': 'A=твор',\n",
       " 'СНАРЯЖЕНИЕМ': 'S',\n",
       " 'СОДЕРЖА': 'V',\n",
       " 'ИСТОРИИ': 'S',\n",
       " 'МАТ': 'S',\n",
       " 'ДРЕВНЕЕГИПЕТСКИЕ': 'A=(вин',\n",
       " 'СИМВОЛЫ': 'S',\n",
       " 'ПОДПИСЬ': 'S',\n",
       " 'СОЗДАВШЕГО': 'V',\n",
       " 'НАБОР': 'S',\n",
       " 'ВАСИ': 'S',\n",
       " 'ПУПКИНА': 'S',\n",
       " 'ВЫБРОСИВ': 'V=прош',\n",
       " 'ВСЁ': 'SPRO',\n",
       " 'ЭТО': 'SPRO',\n",
       " 'ДЕРЬМО': 'S',\n",
       " 'БЛИЖАЙШУЮ': 'A=вин',\n",
       " 'ЛАВОМУСОРКУ': 'S',\n",
       " 'РЕШАЕШЬ': 'V',\n",
       " 'ПРОЙТИ': 'V',\n",
       " 'БЛИЖАЙШЕМУ': 'A=(дат',\n",
       " 'ВЫХОДУ': 'S',\n",
       " 'ПЕРЕСЕЧЬ': 'V=инф',\n",
       " 'ГРАНИЦУ': 'S',\n",
       " 'СПАВНА': 'S',\n",
       " 'ДАБЫ': 'CONJ=',\n",
       " 'ПОЙТИ': 'V',\n",
       " 'ДОБЫВАТЬ': 'V=инф',\n",
       " 'СТРОИТЬ': 'V',\n",
       " 'ДОМ': 'S',\n",
       " 'СТОИЛО': 'V',\n",
       " 'ТЕБЕ': 'SPRO',\n",
       " 'МИЛЛИМЕТР': 'S',\n",
       " 'ОТОЙТИ': 'V',\n",
       " 'ЗОНЫ': 'S',\n",
       " 'ДЕСАНТИРОВАЛИСЬ': 'V',\n",
       " 'ДЕСЯТЬ': 'NUM=(вин|им)',\n",
       " 'ДОНАТЕРОВ': 'S',\n",
       " 'СВЕТЯЩЕЙСЯ': 'V',\n",
       " 'ВСЕМИ': 'SPRO',\n",
       " 'ЦВЕТАМИ': 'S',\n",
       " 'РАДУГИ': 'S',\n",
       " 'АЛМАЗНОЙ': 'A=(пр',\n",
       " 'БРОНЕ': 'S',\n",
       " 'СО': 'PR=',\n",
       " 'СТЕРЖНЕМ': 'S',\n",
       " 'КРАЯ': 'S',\n",
       " 'ВО': 'PART=',\n",
       " 'ЛБУ': 'S',\n",
       " 'НЕ': 'PART=',\n",
       " 'ПОНИМАЯ': 'V=непрош',\n",
       " 'ПРОИСХОДИТ': 'V',\n",
       " 'ЗА': 'PR=',\n",
       " 'МГНОВЕНИЕ': 'S',\n",
       " 'ПОЛУЧАЕШЬ': 'V',\n",
       " 'ЛЮЛЕЙ': 'S',\n",
       " 'КИЛЛАУРЩИКОВ': 'S',\n",
       " 'УЛЮЛЮКАНЬЕ': 'S',\n",
       " 'БЕГАЮЩИХ': 'V',\n",
       " 'ВОКРУГ': 'ADV=',\n",
       " 'ДЕГЕНЕРАТОВ': 'S',\n",
       " 'ОДИНАКОВЫМИ': 'A=твор',\n",
       " 'РОЖАМИ': 'S',\n",
       " 'СКИНАХ': 'S',\n",
       " 'ДОЛБАНУТОЙ': 'V',\n",
       " 'ОДЕЖДЕ': 'S',\n",
       " 'ВСЯ': 'APRO=им',\n",
       " 'КАРТА': 'S',\n",
       " 'ПОКРЫТА': 'V=прош',\n",
       " 'КОРОБКАМИ': 'S',\n",
       " 'АЛМАЗОВ': 'S',\n",
       " 'РАКЕТАМИ': 'S',\n",
       " 'ГРЯЗИ': 'S',\n",
       " 'ГЛИНЫ': 'S',\n",
       " 'НЕБЕ': 'S',\n",
       " 'ГЕКТАРЫ': 'S',\n",
       " 'ПЛОСКИХ': 'A=(пр',\n",
       " 'ОСТРОВОВ': 'S',\n",
       " 'СОДЕРЖАЩИЕ': 'V',\n",
       " 'СЕБЕ': 'SPRO=(пр|дат)',\n",
       " 'МАКСИМУМ': 'S',\n",
       " 'УКРАДЕННЫЙ': 'V',\n",
       " 'ЮТУБА': 'S',\n",
       " 'МОДЕРН': 'S',\n",
       " 'КОНСТРУКЦИЮ': 'S',\n",
       " 'ДЛЯ': 'PR=',\n",
       " 'КОНКУРСА': 'S',\n",
       " 'СКИНОВ': 'S',\n",
       " 'ПРОБРАВШИСЬ': 'V',\n",
       " 'ТАКИ': 'PART=',\n",
       " 'ЧЕРЕЗ': 'PR=',\n",
       " 'МИНИМАЛЬНО': 'ADV=',\n",
       " 'СВОБОДНУЮ': 'A=вин',\n",
       " 'ГЕНИЯ': 'S',\n",
       " 'МОЛОДОГО': 'A=(вин',\n",
       " 'ПОКОЛЕНИЯ': 'S',\n",
       " 'ТЕРРИТОРИЮ': 'S',\n",
       " 'НАЧИНАЕШЬ': 'V',\n",
       " 'ЛАМПОВЫЙ': 'A=(вин',\n",
       " 'ДОМИК': 'S',\n",
       " 'ПЫТАЕШЬСЯ': 'V',\n",
       " 'ПОПРОСИТЬ': 'V',\n",
       " 'ПОМОЩИ': 'S',\n",
       " 'ЧАТЕ': 'S',\n",
       " 'ЖАЛУЯСЬ': 'V',\n",
       " 'НЕОПРАВДАННУЮ': 'A=вин',\n",
       " 'АГРЕССИЮ': 'S',\n",
       " 'ТВОЕЙ': 'APRO=(пр',\n",
       " 'ПЕРСОНЕ': 'S',\n",
       " 'БАН': 'S',\n",
       " 'РАНДОМНОГО': 'S',\n",
       " 'ДОНАТЕРА': 'S',\n",
       " 'ЧИТЫ': 'S'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что некоторые части речи нужно еще почистить от грамматических меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in mys_dict:\n",
    "    pos = mys_dict[w].split('=')[0]\n",
    "    mys_dict[w] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NUM'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys_dict['ДЕСЯТЬ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphVocab()\n",
    " \n",
    "nat_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in nltk.word_tokenize(rus_text):\n",
    "    word = w.split('_')[0]\n",
    "    if word.isalpha():\n",
    "        pos = m(word)[0].pos\n",
    "        nat_dict[word] = pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем словарь соответствий тегов частей речи для 2 парсеров (при разметке вручную я использовала теги PyMorphy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_mys = {'A': ['ADJF', 'ADJS'],\n",
    "            'S': ['NOUN'],\n",
    "            'V': ['VERB', 'PRTF', 'PRTS', 'GRND', 'INFN'],\n",
    "            'ADV': ['ADVB'],\n",
    "            'ANUM': ['NUM'],\n",
    "            'CONJ': ['CONJ'],\n",
    "            'INTJ': ['INTJ'],\n",
    "            'NUM': ['NUM'],\n",
    "            'PART': ['PRCL'],\n",
    "            'SPRO': ['NPRO'],\n",
    "            'PR': ['PREP'],\n",
    "            'APRO': ['ADJF'],\n",
    "            'ADVPRO': ['ADV'],\n",
    "            'None': ['None']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_nat = {'ADJ': ['ADJF', 'ADJS'],\n",
    "            'NOUN': ['NOUN'],\n",
    "            'VERB': ['VERB', 'PRTF', 'PRTS', 'GRND', 'INFN'],\n",
    "            'ADV': ['ADVB'],\n",
    "            'NUM': ['NUM'],\n",
    "            'CCONJ': ['CONJ'],\n",
    "            'INTJ': ['INTJ'],\n",
    "            'NUM': ['NUMR'],\n",
    "            'PART': ['PRCL'],\n",
    "            'PRON': ['NPRO'],\n",
    "            'ADP': ['PREP'],\n",
    "            'X': ['None']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_pym = {'ADJF': ['ADJF'],\n",
    "                'ADJS': ['ADJS'],\n",
    "                'NOUN': ['NOUN'],\n",
    "                'VERB': ['VERB'], \n",
    "                'PRTF': ['PRTF'], \n",
    "                'PRTS': ['PRTS'], \n",
    "                'GRND': ['GRND'], \n",
    "                'INFN': ['INFN'],\n",
    "                'ADVB': ['ADVB'],\n",
    "                'NUM': ['NUM'],\n",
    "                'CONJ': ['CONJ'],\n",
    "                'INTJ': ['INTJ'],\n",
    "                'NUMR': ['NUMR'],\n",
    "                'PRCL': ['PRCL'],\n",
    "                'NPRO': ['NPRO'],\n",
    "                'PREP': ['PREP'],\n",
    "                None: ['None']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(dictionary, alignment):\n",
    "    correct = 0\n",
    "    for word in dictionary:\n",
    "        if pos_dict[word] in alignment[dictionary[word]]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(word, pos_dict[word], dictionary[word])  #  выводим случаи, где разметки не совпали\n",
    "    return correct / len(pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy MyStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В PREP S\n",
      "И CONJ PART\n",
      "К PREP S\n",
      "С PREP S\n",
      "КАК CONJ ADVPRO\n",
      "ВСЕ ADJF SPRO\n",
      "ВСЕХ NOUN SPRO\n",
      "НА PREP PART\n",
      "ПВП NOUN None\n",
      "ВНУТРИ PREP ADV\n",
      "КОТОРОГО CONJ APRO\n",
      "ПОД PREP S\n",
      "ЧТО CONJ SPRO\n",
      "ВСЁ ADJF SPRO\n",
      "ЭТО ADJF SPRO\n",
      "ДЕСЯТЬ NUMR NUM\n",
      "ВСЕМИ ADJF SPRO\n",
      "ВО PREP PART\n",
      "ВОКРУГ PREP ADV\n",
      "ДОЛБАНУТОЙ ADJF V\n",
      "МОДЕРН ADJF S\n",
      "РАНДОМНОГО ADJF S\n",
      "0.8763440860215054\n"
     ]
    }
   ],
   "source": [
    "print(check_accuracy(mys_dict, pos_tags_mys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ВСЕХ NOUN ADJ\n",
      "ПВП NOUN X\n",
      "КОТОРОГО CONJ ADJ\n",
      "ЗАБИТ PRTS ADJ\n",
      "НАБИТЫЕ PRTF ADJ\n",
      "ЭТО ADJF PART\n",
      "СПАВНА NOUN ADJ\n",
      "ДАБЫ CONJ NOUN\n",
      "СВЕТЯЩЕЙСЯ PRTF ADJ\n",
      "ВОКРУГ PREP ADV\n",
      "ДОЛБАНУТОЙ ADJF NOUN\n",
      "МОДЕРН ADJF NOUN\n",
      "СКИНОВ NOUN ADJ\n",
      "БАН NOUN X\n",
      "РАНДОМНОГО ADJF ADV\n",
      "0.9139784946236559\n"
     ]
    }
   ],
   "source": [
    "print(check_accuracy(nat_dict, pos_tags_nat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy PyMorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ВСЕХ NOUN ADJF\n",
      "ПВП NOUN None\n",
      "КОТОРОГО CONJ ADJF\n",
      "ЗАБИТ PRTS ADJS\n",
      "НАБИТЫЕ PRTF ADJF\n",
      "ЭТО ADJF PRCL\n",
      "СПАВНА NOUN ADJS\n",
      "ДАБЫ CONJ NOUN\n",
      "СВЕТЯЩЕЙСЯ PRTF ADJF\n",
      "ВОКРУГ PREP ADVB\n",
      "ДОЛБАНУТОЙ ADJF NOUN\n",
      "МОДЕРН ADJF NOUN\n",
      "СКИНОВ NOUN ADJS\n",
      "БАН NOUN None\n",
      "РАНДОМНОГО ADJF ADVB\n",
      "0.9139784946236559\n"
     ]
    }
   ],
   "source": [
    "print(check_accuracy(pym_dict, pos_tags_pym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Английский текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng_text.txt', encoding='utf-8') as f:\n",
    "    eng_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_eng_dict = {}  #  считываем собственную разметку\n",
    "\n",
    "for w in nltk.word_tokenize(eng_text):\n",
    "    if w[0].isalpha():\n",
    "        word, pos = w.split('_')\n",
    "        pos_eng_dict[word] = pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы делаем pos-tagging тремя парсерами по очереди и делаем для них словари"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc_dict = {}\n",
    "\n",
    "tokens = nltk.word_tokenize(eng_text)\n",
    "text = ''\n",
    "for t in tokens:\n",
    "    text += t.split('_')[0] + ' '\n",
    "\n",
    "doc = nlp(text)\n",
    "for w in doc:\n",
    "    if w.text[0].isalpha():\n",
    "        spc_dict[w.text] = w.pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_dict = {}\n",
    "\n",
    "tokens = nltk.word_tokenize(eng_text)\n",
    "text = ''\n",
    "for t in tokens:\n",
    "    text += t.split('_')[0] + ' '\n",
    "for w in nltk.pos_tag(nltk.word_tokenize(text)):\n",
    "    nltk_dict[w[0]] = w[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-17 13:15:36,165 loading file C:\\Users\\Анастасия\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(eng_text)\n",
    "text = ''\n",
    "for t in tokens:\n",
    "    if t[0].isalpha():\n",
    "        text += t.split('_')[0] + ' '\n",
    "\n",
    "sentence = Sentence(text)\n",
    "tagger = SequenceTagger.load('pos')\n",
    "tagger.predict(sentence)\n",
    "\n",
    "\n",
    "parsed = sentence.to_tagged_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_dict = {}\n",
    "\n",
    "for w in parsed.split()[::2]:\n",
    "    fl_dict[w] = parsed.split()[parsed.split().index(w) + 1].strip('<>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mister <NNP> Bennet <NNP> was <VBD> so <RB> odd <JJ> a <DT> mixture <NN> of <IN> quick <JJ> parts <NNS> sarcastic <JJ> humour <NN> reserve <NN> and <CC> caprice <NN> that <IN> the <DT> experience <NN> of <IN> three <CD> and <CC> twenty <CD> years <NNS> had <VBD> been <VBN> insufficient <JJ> to <TO> make <VB> his <PRP$> wife <NN> understand <VB> his <PRP$> character <NN> Her <PRP$> mind <NN> was <VBD> less <RBR> difficult <JJ> to <TO> develop <VB> She <PRP> was <VBD> a <DT> woman <NN> of <IN> mean <JJ> understanding <VBG> little <JJ> information <NN> and <CC> uncertain <JJ> temper <NN> When <WRB> she <PRP> was <VBD> discontented <VBN> she <PRP> fancied <VBD> herself <PRP> nervous <JJ> The <DT> business <NN> of <IN> her <PRP$> life <NN> was <VBD> to <TO> get <VB> her <PRP$> daughters <NNS> married <VBN> its <PRP$> solace <NN> was <VBD> visiting <VBG> and <CC> news <NN> Bennet <NNP> was <VBD> among <IN> the <DT> earliest <JJS> of <IN> those <DT> who <WP> waited <VBD> on <IN> Bingley <NNP> He <PRP> had <VBD> always <RB> intended <VBN> to <TO> visit <VB> him <PRP> though <RB> to <IN> the <DT> last <JJ> always <RB> assuring <VBG> his <PRP$> wife <NN> that <IN> he <PRP> should <MD> not <RB> go <VB> and <CC> till <IN> the <DT> evening <NN> after <IN> the <DT> visit <NN> was <VBD> paid <VBN> she <PRP> had <VBD> no <DT> knowledge <NN> of <IN> it <PRP> It <PRP> was <VBD> then <RB> disclosed <VBN> in <IN> the <DT> following <VBG> manner <NN> Observing <VBG> his <PRP$> second <JJ> daughter <NN> employed <VBN> in <IN> trimming <VBG> a <DT> hat <NN> he <PRP> suddenly <RB> addressed <VBD> her <PRP> with <IN> I <PRP> hope <VBP> Bingley <NNP> will <MD> like <VB> it <PRP> Lizzy <NNP>'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае для разметки мы использовали теги Spacy, напишем словари соответствий для других теггеров (оказалось, что NLTK и FLair используют один набор тегов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_nltk = {\n",
    "    'NNP': ['PROPN'],\n",
    "    'NN': ['NOUN'],\n",
    "    'NNS': ['NOUN'],\n",
    "    'VB': ['VERB'],\n",
    "    'VBD': ['VERB'],\n",
    "    'VBN': ['VERB'],\n",
    "    'VBG': ['VERB'],\n",
    "    'VBP': ['VERB'],\n",
    "    'JJ': ['ADJ'],\n",
    "    'JJS': ['ADJ'],\n",
    "    'JJR': ['ADJ'],\n",
    "    'PRP': ['PRON'],\n",
    "    'WP': ['PRON'],\n",
    "    'IN': ['ADP'],\n",
    "    'DT': ['DET'],\n",
    "    'RB': ['ADV'],\n",
    "    'RBR': ['ADV'],\n",
    "    'RBS': ['ADV'],\n",
    "    'WRB': ['ADV'],\n",
    "    'PRP$': ['DET'],\n",
    "    'MD': ['AUX'],\n",
    "    'TO': ['PART'],\n",
    "    'CD': ['NUM'],\n",
    "    'CC': ['CCONJ']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_spc = {\n",
    "    'PROPN': ['PROPN'],\n",
    "    'NOUN': ['NOUN'],\n",
    "    'VERB': ['VERB'],\n",
    "    'ADJ': ['ADJ'],\n",
    "    'PRON': ['PRON'],\n",
    "    'ADP': ['ADP'],\n",
    "    'DET': ['DET'],\n",
    "    'ADV': ['ADV'],\n",
    "    'DET': ['DET'],\n",
    "    'AUX': ['AUX'],\n",
    "    'PART': ['PART'],\n",
    "    'NUM': ['NUM'],\n",
    "    'CCONJ': ['CCONJ'],\n",
    "    'SCONJ': ['SCONJ']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Считаем Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(dictionary, alignment):\n",
    "    correct = 0\n",
    "    for word in dictionary:\n",
    "        if pos_eng_dict[word] in alignment[dictionary[word]]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(word, pos_eng_dict[word], dictionary[word])  #  выводим случаи, где разметки не совпали\n",
    "    return correct / len(pos_eng_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was AUX VBD\n",
      "that SCONJ IN\n",
      "had AUX VBD\n",
      "been AUX VBN\n",
      "to ADP TO\n",
      "understanding NOUN JJ\n",
      "get AUX VB\n",
      "though ADV IN\n",
      "should VERB MD\n",
      "not PART RB\n",
      "till SCONJ VB\n",
      "following VERB JJ\n",
      "will VERB MD\n",
      "0.8773584905660378\n"
     ]
    }
   ],
   "source": [
    "print(check_accuracy(nltk_dict, pos_tags_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was AUX VBD\n",
      "that SCONJ IN\n",
      "had AUX VBD\n",
      "been AUX VBN\n",
      "to ADP TO\n",
      "understanding NOUN VBG\n",
      "her PRON PRP$\n",
      "get AUX VB\n",
      "visit NOUN VB\n",
      "should VERB MD\n",
      "not PART RB\n",
      "till SCONJ IN\n",
      "will VERB MD\n",
      "0.8773584905660378\n"
     ]
    }
   ],
   "source": [
    "print(check_accuracy(fl_dict, pos_tags_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humour NOUN PROPN\n",
      "understanding NOUN ADJ\n",
      "0.9811320754716981\n"
     ]
    }
   ],
   "source": [
    "print(check_accuracy(spc_dict, pos_tags_spc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2. Улучшение предсказаний второй домашки путем новой обработки текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки возьмем Наташу, которая показала высокую accuracy на данных выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Группы, которые мы хотим выделять:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Не + глагол в 1 лице (не рекомендую, не советую)\n",
    "2. Личное местоимение/существительное в дативе + нравиться/понравиться в 3sg или 3pl (ловим сочетания \"нам нравится\", \"мне нравятся\", \"ему (ребенку) понравилось\")\n",
    "3. Краткое причастие + наречие (ловим сочетания типа 'хорошо сделано', 'красиво оформлена' или 'плохо нарисовано')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=3, text='Мне', id='1_1', head_id='1_2', rel='iobj', pos='PRON', feats=<Dat,Sing,1>),\n",
       " DocToken(start=4, stop=15, text='понравилось', id='1_2', head_id='1_0', rel='root', pos='VERB', feats=<Perf,Neut,Ind,Sing,Past,Fin,Mid>),\n",
       " DocToken(start=15, stop=16, text=',', id='1_3', head_id='1_6', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=17, stop=24, text='ребенку', id='1_4', head_id='1_2', rel='iobj', pos='NOUN', feats=<Anim,Dat,Masc,Sing>),\n",
       " DocToken(start=25, stop=27, text='не', id='1_5', head_id='1_6', rel='advmod', pos='PART', feats=<Neg>),\n",
       " DocToken(start=28, stop=36, text='нравится', id='1_6', head_id='1_2', rel='conj', pos='VERB', feats=<Imp,Ind,Sing,3,Pres,Fin,Mid>),\n",
       " DocToken(start=36, stop=37, text=',', id='1_7', head_id='1_6', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=38, stop=47, text='выполнено', id='1_8', head_id='1_2', rel='conj', pos='VERB', feats=<Perf,Neut,Sing,Past,Short,Part,Pass>),\n",
       " DocToken(start=48, stop=53, text='очень', id='1_9', head_id='1_10', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=54, stop=60, text='хорошо', id='1_10', head_id='1_8', rel='advmod', pos='ADV', feats=<Pos>)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ┌► Мне         iobj\n",
      "┌─┌───┌─└─ понравилось \n",
      "│ │ ┌►│    ,           punct\n",
      "│ │ │ └──► ребенку     iobj\n",
      "│ │ │   ┌► не          advmod\n",
      "│ └►└─┌─└─ нравится    conj\n",
      "│     └──► ,           punct\n",
      "└────►┌─── выполнено   conj\n",
      "      │ ┌► очень       advmod\n",
      "      └►└─ хорошо      advmod\n"
     ]
    }
   ],
   "source": [
    "nat = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "segmenter = Segmenter()\n",
    "\n",
    "doc = Doc('Мне понравилось, ребенку не нравится, выполнено очень хорошо')\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "display(doc.tokens)\n",
    "doc.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для поиска 1 группы в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gr_one(doc):\n",
    "    groups = []\n",
    "    for d in doc.tokens:\n",
    "        if d.text == 'не':\n",
    "            head = d.head_id\n",
    "            for h in doc.tokens:\n",
    "                if h.id == head and h.feats['Number'] == 'Sing':\n",
    "                    groups.append(d.text + ' ' + h.text)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для поиска 2 группы (берем только группу без \"не\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gr_two(doc):\n",
    "    groups = []\n",
    "    for d in doc.tokens:\n",
    "        if d.pos == 'VERB' and nat(d.text)[0].normal == 'нравиться' or nat(d.text)[0].normal == 'понравиться':\n",
    "            head = d.id\n",
    "            is_neg = 0\n",
    "            for h in doc.tokens:\n",
    "                if h.head_id == head and h.text == 'не':\n",
    "                    is_neg = 1\n",
    "                if h.head_id == head and 'Case' in h.feats and h.feats['Case'] == 'Dat':\n",
    "                    group = h.text + ' ' + d.text\n",
    "            if not is_neg:\n",
    "                groups.append(group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для поиска 3 группы в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gr_three(doc):\n",
    "    groups = []\n",
    "    for d in doc.tokens:\n",
    "        if d.pos == 'VERB' and d.feats['VerbForm'] == 'Part' and d.feats['Variant'] == 'Short':\n",
    "            head = d.id\n",
    "            for h in doc.tokens:\n",
    "                if h.head_id == head and h.pos == 'ADV':\n",
    "                    groups.append(d.text + ' ' + h.text)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['не нравится']\n",
      "['ребенку понравилось']\n",
      "['выполнено хорошо']\n"
     ]
    }
   ],
   "source": [
    "text = 'Мне понравилось, ребенку не нравится, выполнено очень хорошо'\n",
    "\n",
    "\n",
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "    \n",
    "print(find_gr_one(doc))\n",
    "print(find_gr_two(doc))\n",
    "print(find_gr_three(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все это можно использовать при предсказании (хотя видим, что синтаксический парсинг не идеальный - Наташа выделяет \"ребенку\" как зависимое от слова \"понравилось\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
